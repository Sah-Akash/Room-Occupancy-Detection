{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Room Occupancy-Detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPX9AxtOXEMLQfsLCo5qLLC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"GAZKaGyBdLPs","executionInfo":{"status":"ok","timestamp":1623774200041,"user_tz":-330,"elapsed":1350,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["from __future__ import division\n","import numpy as np\n","import pandas as pd"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYe63tB4e_p6","executionInfo":{"status":"ok","timestamp":1623774221196,"user_tz":-330,"elapsed":1421,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.preprocessing import MinMaxScaler"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9Rb428gfE0o","executionInfo":{"status":"ok","timestamp":1623774230603,"user_tz":-330,"elapsed":841,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqz3OntGfHHj","executionInfo":{"status":"ok","timestamp":1623774241039,"user_tz":-330,"elapsed":411,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"bd0l_4lvfLrY","executionInfo":{"status":"ok","timestamp":1623774253852,"user_tz":-330,"elapsed":405,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from mlxtend.classifier import StackingClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"r46pVx9FfNIt","executionInfo":{"status":"ok","timestamp":1623774271845,"user_tz":-330,"elapsed":410,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["from sklearn.pipeline import make_pipeline\n","\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n","from sklearn.metrics import roc_curve, average_precision_score, matthews_corrcoef, make_scorer\n","\n","from sklearn.feature_selection import SelectKBest,chi2, f_classif, mutual_info_classif, VarianceThreshold"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SmCJMkXxfSaF","executionInfo":{"status":"ok","timestamp":1623774464929,"user_tz":-330,"elapsed":420,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}}},"source":["train_data = pd.read_csv(\"/content/SpaData.csv\")\n","\n","y_train = train_data['GTOccupancy']\n","X_train = train_data.loc[:, train_data.columns != 'GTOccupancy']\n","\n","X_train_two, X_validation, y_train_two, y_validation = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train, random_state=42)\n","\n","test_data = pd.read_csv(\"/content/SpaData.csv\") # My dataset has two test sets. This is test1.\n","\n","y_test = test_data['GTOccupancy']\n","X_test = test_data.loc[:, test_data.columns != 'Occupancy']\n","\n","test_data2 = pd.read_csv(\"/content/SpaData.csv\") # My dataset has two test sets. This is test1.\n","\n","y_test2 = test_data2['GTOccupancy']\n","X_test2 = test_data2.loc[:, test_data2.columns != 'GTOccupancy']\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"_knNN8D7gCcs","executionInfo":{"status":"error","timestamp":1623774500820,"user_tz":-330,"elapsed":1681,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}},"outputId":"c5456ece-c866-4ae5-f719-0772dfa61a97"},"source":["# ### 3. Normalize features in your train set 2 and validation set using min-max scaling to interval [0,1]. For this purpose you can first normalize features in your train set 2 and use the same scaling coefficients to normalize validation set. Save the normalized versions as separate files. Repeat normalizing your original train set and use the same normalization coefficients to normalize the two test sets.\n","\n","scaler = MinMaxScaler()\n","\n","scaler.fit(X_train_two)\n","normalized_x_train_two = scaler.transform(X_train_two)\n","normalized_x_validation_train_two = scaler.transform(X_validation)\n","\n","np.savetxt(\"normalized_x_train_two.csv\", normalized_x_train_two, delimiter=\",\")\n","np.savetxt(\"normalized_x_validation.csv\", normalized_x_validation_train_two, delimiter=\",\")\n","\n","scaler.fit(X_train)\n","normalized_x_train = scaler.transform(X_train)\n","normalized_x_test = scaler.transform(X_test)\n","normalized_x_test2 = scaler.transform(X_test2)\n","normalized_x_validation_with_orig = scaler.transform(X_validation)"],"execution_count":9,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c6addca474ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnormalized_x_train_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnormalized_x_validation_train_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    371\u001b[0m         X = check_array(X,\n\u001b[1;32m    372\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: '06-02-15 20:21'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"jqa-jdbVgWLZ","executionInfo":{"status":"error","timestamp":1623774563821,"user_tz":-330,"elapsed":739,"user":{"displayName":"Aayan Sah","photoUrl":"","userId":"17596488976369731194"}},"outputId":"3e2e657c-a43c-4616-8f3a-a07d36442827"},"source":["# ### 4. Perform a 10-fold cross-validation experiment for the random forest classifier on normalized and unnormalized versions of train set 2. You can set the number of trees to 100. Do you get better accuracy when you perform data normalization?\n","\n","clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","kfold = StratifiedKFold(n_splits=10, random_state=42)\n","\n","unnormalized_accuracy = cross_val_score(clf, X_train_two, y_train_two, cv=kfold).mean()\n","normalized_accuracy = cross_val_score(clf, normalized_x_train_two, y_train_two, cv=kfold).mean()\n","\n","if unnormalized_accuracy >= normalized_accuracy:\n","    print(\"No, I did not. Unnormalized Accuracy: {}, Normalized Accuracy: {}\"\n","          .format(unnormalized_accuracy,normalized_accuracy))\n","else:\n","    print(\"Yes, I did. Unnormalized Accuracy: {}, Normalized Accuracy: {}\"\n","          .format(unnormalized_accuracy,normalized_accuracy))\n"],"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-7ccf64f39bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0munnormalized_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnormalized_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_x_train_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munnormalized_accuracy\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnormalized_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'normalized_x_train_two' is not defined"]}]},{"cell_type":"code","metadata":{"id":"2vUSPXyLgZqM"},"source":["# ### 5. Perform a 10-fold cross-validation experiment on train set 2 that corresponds to the best performing normalization strategy (i.e. normalized or unnormalized) for the following classifiers: \n","# \n","# Logistic regression\n","# \n","# k-nearest neighbor (with k=1)\n","# \n","# Naïve Bayes\n","# \n","# Decision tree\n","# \n","# Random forest (number of trees=100)\n","# \n","# SVM (RBF kernel C=1.0 gamma=0.125)\n","# \n","# Linear Discriminant Analysis (This model is used instead of RBF Network)\n","# \n","# Adaboost (number of iterations=10)\n","# \n","# You can use default values for other hyper-parameters of the classifiers\n","# Report the following accuracy measures for each of these classifiers: overall\n","# accuracy, F-measure, sensitivity, specificity, precision, area under the ROC curve,\n","# area under the precision recall curve, MCC scores. These will be cross-validation\n","# accuracies.\n","\n","models = [\n","    (\"Logistic Regression\",LogisticRegression(random_state=42)),\n","    (\"K-Nearest Neighbour\",KNeighborsClassifier(n_neighbors=1)),\n","    (\"Naive Bayes\",GaussianNB()),\n","    (\"Decision Tree\",DecisionTreeClassifier(random_state=42)),\n","    (\"Random Forest\",RandomForestClassifier(n_estimators=100, random_state=42)),\n","    (\"Support Vector Machine\",SVC(kernel=\"rbf\", C=1,gamma=0.125, random_state=42)),\n","    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n","    (\"AdaBoostClassifier\",AdaBoostClassifier(n_estimators=10, random_state=42))\n","]\n","\n","metrics = [\n","    (\"Accuracy Score\", accuracy_score),\n","    (\"F-Measure\", f1_score),\n","    (\"Sensitivity\", recall_score),\n","    (\"Specificity\", recall_score),\n","    (\"Precision\", precision_score),\n","    (\"Area Under ROC Curve\", roc_auc_score),\n","    (\"Area Under Precision Recall Curve\", average_precision_score),\n","    (\"MCC\", matthews_corrcoef)\n","]\n","\n","def dump_metrics_with_cv(features, label, name, model):\n","    accuracy = 0.0\n","    \n","    for metric_name, metric in metrics:\n","        kfold = StratifiedKFold(n_splits=10, random_state=42)\n","        scorer = make_scorer(metric,pos_label=0) if metric_name == \"Specificity\" else make_scorer(metric)\n","\n","        cv_result = cross_val_score(model,features,label.values.ravel(), cv = kfold,scoring = scorer)\n","        \n","        if metric_name == \"Accuracy Score\":\n","            accuracy = cv_result\n","\n","        print(\"{} {}: {}\".format(name, metric_name, cv_result.mean()))\n","    print(\"\")\n","    \n","    return accuracy.mean()\n","\n","for name,model in models:\n","    print(\"Unnormalized Results for {}:\\n\".format(name))\n","    dump_metrics_with_cv(X_train_two, y_train_two, name, model)\n","    \n","    print(\"\\nNormalized Results for {}:\\n\".format(name))\n","    dump_metrics_with_cv(normalized_x_train_two, y_train_two, name, model)\n","\n","\n","# ### 6. Use three feature selection methods to select feature subsets on train set 2 and compute accuracy measures in step 5 for all the classifiers. Repeat for normalized version of train set 2. Do you get improvement in accuracy when you perform feature selection or is it better to use all of the features? Which feature selection strategy gives the best accuracy?\n","\n","for name, model in models:\n","    selectors = [\n","        (\"VarianceThreshold\", VarianceThreshold()),\n","        (\"SelectKBest with f_classif 1\", SelectKBest(f_classif,k=1)),\n","        (\"SelectKBest with chi2 1\", SelectKBest(chi2,k=1)),\n","        (\"SelectKBest with f_classif 2\", SelectKBest(f_classif,k=2)),\n","        (\"SelectKBest with chi2 2\", SelectKBest(chi2,k=2)),\n","        (\"SelectKBest with f_classif 3\", SelectKBest(f_classif,k=3)),\n","        (\"SelectKBest with chi2 3\", SelectKBest(chi2,k=3)),\n","        (\"SelectKBest with f_classif 4\", SelectKBest(f_classif,k=4)),\n","        (\"SelectKBest with chi2 4\", SelectKBest(chi2,k=4)),\n","        (\"SelectKBest with f_classif 5\", SelectKBest(f_classif,k=5)),\n","        (\"SelectKBest with chi2 5\", SelectKBest(chi2,k=5)),\n","        (\"SelectKBest with f_classif 6\", SelectKBest(f_classif,k=6)),\n","        (\"SelectKBest with chi2 7\", SelectKBest(chi2,k=6)),\n","        (\"SelectKBest with f_classif 7\", SelectKBest(f_classif,k=7)),\n","        (\"SelectKBest with chi2 7\", SelectKBest(chi2,k=7))\n","    ]\n","    \n","    for selector_name, selector in selectors:  \n","        selected_x_train_two = selector.fit_transform(X_train_two, y=y_train_two)\n","        normalized_selected_x_train_two  = selector.fit_transform(normalized_x_train_two, y=y_train_two)\n","\n","        print(\"{} unnormalized results:\\n\".format(selector_name))\n","        dump_metrics_with_cv(selected_x_train_two, y_train_two, name, model)\n","\n","        print(\"{} normalized results:\\n\".format(selector_name))\n","        dump_metrics_with_cv(normalized_selected_x_train_two, y_train_two, name, model)\n","\n","print(\"Yes, I did get improvement when I performed feature selection.\")\n","print(\"SelectKBest that uses chi2 with 4 features gave the best accuracy.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZlV6zWYgr0k"},"source":["# ### 7. Choose the version of train set 2 that contains the optimum feature set you found in step 6 and the data for the best normalization strategy. Optimize the following hyperparameters:\n","# \n","# k parameter in k-NN\n","# number of trees in random forest\n","# Number of iterations in Adaboost\n","# C, gamma parameter in SVM\n","# \n","# Try a grid of values and choose the best value(s) that maximize the overall cross validation accuracy.\n","# \n","# * For k-NN you can choose 1 5 10 15 ... 100 (with increments of 5 after k=5)\n","# * For number of trees in random forest you can try 5 10 25 50 75 100 150 200 250 300 350 400 450 500\n","# * For the number of iterations in Adaboost you can try 5 10 15 20 25 30 40 50 75 100 125 150 175 200\n","# \n","# To optimize C and gamma parameters of the SVM you can consider the\n","# following parameter grid:\n","# \n","# C ∈ {2^-5, 2^-3, 2^-1, 2^1, 2^3, 2^5, ... 2^13, 2^15}\n","# γ ∈ {2^-15, 2^-13, ... , 2^-1, 2^1, 2^3, 2^5}\n","# \n","# There are a total of 11 values for the C parameter and 11 values for the gamma parameter (a total of 121 values to consider for the (C, gamma) pair).\n","# \n","# Report the best cross-validation accuracies and optimum parameter values you found.\n","# \n","# Compute predictions on the validation set using the models trained by optimum\n","# hyper-parameters. Report the same accuracy measures as in step 5.\n","\n","best_selector = SelectKBest(chi2,k=4)\n","best_selector.fit(normalized_x_train_two, y=y_train_two)\n","optimum_x_train_two = best_selector.transform(normalized_x_train_two)\n","\n","hyper_parameters = {\n","    \"K-Nearest Neighbour\": [1] + [i*5 for i in range(1,21)],\n","    \"Random Forest\": [5,10,25,50,75,100,150,200,250,300,350,400,450,500],\n","    \"AdaBoostClassifier\": [5,10,15,20,25,30,40,50,75,100,125,150,175,200],\n","    \"Support Vector Machine\": ([2**i for i in range(-5,16,2)], [2**i for i in range(-15,6,2)])\n","}\n","\n","opt_params = dict()\n","\n","for model_name, parameters in hyper_parameters.items():\n","    model = None\n","    optimum_accuracy = 0.0\n","    optimum_parameters = \"\"\n","    \n","    if model_name == \"K-Nearest Neighbour\":\n","        for parameter in parameters:\n","            print(\"n_neighbors: {}\".format(parameter))\n","            model = KNeighborsClassifier(n_neighbors=parameter)\n","            accuracy = dump_metrics_with_cv(optimum_x_train_two, y_train_two, model_name, model)\n","            \n","            if accuracy > optimum_accuracy:\n","                optimum_accuracy = accuracy\n","                optimum_parameters = \"n_neighbors=\" + str(parameter)\n","                opt_params[\"n_neighbors\"] = parameter\n","    elif model_name == \"Random Forest\":\n","        for parameter in parameters:\n","            print(\"n_estimators: {}\".format(parameter))\n","            model = RandomForestClassifier(n_estimators=parameter)\n","            accuracy = dump_metrics_with_cv(optimum_x_train_two, y_train_two, model_name, model)\n","            \n","            if accuracy > optimum_accuracy:\n","                optimum_accuracy = accuracy\n","                optimum_parameters = \"n_estimators=\" + str(parameter)\n","                opt_params[\"rf_estimators\"] = parameter\n","    elif model_name == \"Support Vector Machine\":\n","        C_values = parameters[0]\n","        gamma_values = parameters[1]\n","        \n","        for C in C_values:\n","            for gamma in gamma_values:\n","                print(\"C: {}, gamma: {}\".format(C, gamma))\n","                model = SVC(kernel=\"rbf\", C=C,gamma=gamma)\n","                accuracy = dump_metrics_with_cv(optimum_x_train_two, y_train_two, model_name, model)\n","                \n","                if accuracy > optimum_accuracy:\n","                    optimum_accuracy = accuracy\n","                    optimum_parameters = \"C:\" + str(C) + \", \" + \"gamma:\" + str(gamma)\n","                    opt_params[\"svm_c\"] = C\n","                    opt_params[\"svm_gamma\"] = gamma\n","    elif model_name == \"AdaBoostClassifier\":\n","        for parameter in parameters:\n","            print(\"n_estimators: {}\".format(parameter))\n","            model = AdaBoostClassifier(n_estimators=parameter)\n","            accuracy = dump_metrics_with_cv(optimum_x_train_two, y_train_two, model_name, model)\n","            \n","            if accuracy > optimum_accuracy:\n","                optimum_accuracy = accuracy\n","                optimum_parameters = \"n_estimators=\" + str(parameter)\n","                opt_params[\"ab_estimators\"] = parameter\n","    \n","    print(\"Optimum Accuracy for {}: {}\".format(model_name, optimum_accuracy))\n","    print(\"Optimum Parameters:{}\\n\\n\".format(optimum_parameters))\n","\n","\n","classifiers = [\n","    (\"K-Nearest Neighbour\",KNeighborsClassifier(n_neighbors=opt_params[\"n_neighbors\"])),\n","    (\"Random Forest\",RandomForestClassifier(n_estimators=opt_params[\"rf_estimators\"], random_state=42)),\n","    (\"Support Vector Machine\",SVC(kernel=\"rbf\", C=opt_params[\"svm_c\"],gamma=opt_params[\"svm_gamma\"], random_state=42)),\n","    (\"AdaBoostClassifier\",AdaBoostClassifier(n_estimators=opt_params[\"ab_estimators\"], random_state=42))\n","]\n","\n","def dump_metrics_without_cv(features, label, validation, name, model):\n","    model.fit(features, label)\n","    preds = model.predict(validation)\n","    \n","    print(\"Accuracy of {} with optimal hyperparameters on validation set: {}\".format(name, accuracy_score(y_validation, preds)))\n","    print(\"F-Score of {} with optimal hyperparameters on validation set: {}\".format(name, f1_score(y_validation, preds)))\n","    print(\"Sensitivity of {} with optimal hyperparameters on validation set: {}\".format(name, recall_score(y_validation, preds)))\n","    print(\"Specificity of {} with optimal hyperparameters on validation set: {}\".format(name, recall_score(y_validation, preds,pos_label=0)))\n","    print(\"Precision of {} with optimal hyperparameters on validation set: {}\".format(name, precision_score(y_validation, preds)))\n","    print(\"ROC-Auc Score of {} with optimal hyperparameters on validation set: {}\".format(name, roc_auc_score(y_validation, preds)))\n","    print(\"Precision-Recall Score of {} with optimal hyperparameters on validation set: {}\".format(name, average_precision_score(y_validation, preds)))\n","    print(\"MCC of {} with optimal hyperparameters on validation set: {}\\n\\n\".format(name, matthews_corrcoef(y_validation, preds)))\n","\n","optimum_x_validation = best_selector.transform(normalized_x_validation_train_two)\n","    \n","for name,model in classifiers:\n","    dump_metrics_without_cv(optimum_x_train_two, y_train_two, optimum_x_validation, name, model)\n","\n","\n","# ### 8. Implement a stacking ensemble, which combines the best performing classifiers obtained in step 7 by a meta-learner (which can be logistic regression). Here you will use the optimum hyper-parameters you found in step 7 to train the models you selected in stacking. You can try different combinations of classifiers for this purpose. Perform cross-validation and report the same accuracy measures as in step 5. Then train the model on the train set 2 and test on validation set. Report the accuracy measures on validation data.\n","\n","classifiers_for_stacking = [\n","    KNeighborsClassifier(n_neighbors=opt_params[\"n_neighbors\"]),\n","    RandomForestClassifier(n_estimators=opt_params[\"rf_estimators\"], random_state=42),\n","    SVC(kernel=\"rbf\", C=opt_params[\"svm_c\"],gamma=opt_params[\"svm_gamma\"], random_state=42, probability=True),\n","    AdaBoostClassifier(n_estimators=opt_params[\"ab_estimators\"], random_state=42)\n","]\n","\n","lr = LogisticRegression(random_state=42)\n","\n","print(\"Scores with optimum data:\\n\")\n","\n","sclf = StackingClassifier(classifiers=classifiers_for_stacking, meta_classifier=lr)\n","dump_metrics_with_cv(optimum_x_train_two, y_train_two, \"Stacking Ensemble\", sclf)\n","\n","print(\"Scores with validation data:\\n\")\n","dump_metrics_without_cv(optimum_x_train_two, y_train_two, optimum_x_validation, \"Stacking Ensemble\", sclf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bobSY1XNg0k8"},"source":["# ### 9. Generate ROC curves for the methods compared and combine these in a single plot. Comment on the accuracy results. Which methods give the best performance? Can you suggest other methods to further improve the accuracy?\n","\n","result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n","\n","optimal_classifiers = [\n","    KNeighborsClassifier(n_neighbors=opt_params[\"n_neighbors\"]),\n","    RandomForestClassifier(n_estimators=opt_params[\"rf_estimators\"], random_state=42),\n","    SVC(kernel=\"rbf\", C=opt_params[\"svm_c\"],gamma=opt_params[\"svm_gamma\"], random_state=42, probability=True),\n","    AdaBoostClassifier(n_estimators=opt_params[\"ab_estimators\"], random_state=42),\n","    StackingClassifier(classifiers=classifiers_for_stacking, meta_classifier=lr)\n","]\n","\n","for cls in optimal_classifiers:\n","    model = cls.fit(optimum_x_train_two, y_train_two)\n","    yproba = model.predict_proba(optimum_x_validation)[::,1]\n","    \n","    fpr, tpr, _ = roc_curve(y_validation,  yproba)\n","    auc = roc_auc_score(y_validation, yproba)\n","    \n","    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n","                                        'fpr':fpr, \n","                                        'tpr':tpr, \n","                                        'auc':auc}, ignore_index=True)\n","\n","result_table.set_index('classifiers', inplace=True)\n","\n","fig = plt.figure(figsize=(8,6))\n","\n","for i in result_table.index:\n","    plt.plot(result_table.loc[i]['fpr'], \n","             result_table.loc[i]['tpr'], \n","             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n","    \n","plt.plot([0,1], [0,1], color='orange', linestyle='--')\n","\n","plt.xticks(np.arange(0.0, 1.1, step=0.1))\n","plt.xlabel(\"False Positive Rate\", fontsize=15)\n","\n","plt.yticks(np.arange(0.0, 1.1, step=0.1))\n","plt.ylabel(\"True Positive Rate\", fontsize=15)\n","\n","plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n","plt.legend(prop={'size':13}, loc='lower right')\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oA21AM1eg6U7"},"source":["# ### 10. Train the method that gives the most accurate predictions so far (i.e. the highest overall accuracy) on the original train set after applying the best feature selection and normalization strategy and compute predictions on the samples of the test set(s) for which the true labels are available. Report the same accuracy measures as in step 5.\n","\n","clf = RandomForestClassifier(n_estimators=opt_params[\"rf_estimators\"], random_state=42)\n","\n","best_selector.fit(normalized_x_train, y=y_train)\n","optimum_x_train = best_selector.transform(normalized_x_train)\n","optimum_x_test = best_selector.transform(normalized_x_test)\n","optimum_x_test2 = best_selector.transform(normalized_x_test2)\n","\n","clf.fit(optimum_x_train, y_train)\n","\n","preds = clf.predict(optimum_x_test)\n","\n","print(\"Accuracy of RandomForest with optimal hyperparameters on test set: {}\".format(accuracy_score(y_test, preds)))\n","print(\"F-Score of RandomForest with optimal hyperparameters on test set: {}\".format(f1_score(y_test, preds)))\n","print(\"Sensitivity of RandomForest with optimal hyperparameters on test set: {}\".format(recall_score(y_test, preds)))\n","print(\"Specificity of RandomForest with optimal hyperparameters on test set: {}\".format(recall_score(y_test, preds,pos_label=0)))\n","print(\"Precision of RandomForest with optimal hyperparameters on test set: {}\".format(precision_score(y_test, preds)))\n","print(\"ROC-Auc Score of RandomForest with optimal hyperparameters on test set: {}\".format(roc_auc_score(y_test, preds)))\n","print(\"MCC of RandomForest with optimal hyperparameters on test set: {}\\n\\n\".format(matthews_corrcoef(y_test, preds)))\n","\n","preds = clf.predict(optimum_x_test2)\n","\n","print(\"Accuracy of RandomForest with optimal hyperparameters on test set2: {}\".format(accuracy_score(y_test2, preds)))\n","print(\"F-Score of RandomForest with optimal hyperparameters on test set2: {}\".format(f1_score(y_test2, preds)))\n","print(\"Sensitivity of RandomForest with optimal hyperparameters on test set2: {}\".format(recall_score(y_test2, preds)))\n","print(\"Specificity of RandomForest with optimal hyperparameters on test set2: {}\".format(recall_score(y_test2, preds,pos_label=0)))\n","print(\"Precision of RandomForest with optimal hyperparameters on test set2: {}\".format(precision_score(y_test2, preds)))\n","print(\"ROC-Auc Score of RandomForest with optimal hyperparameters on test set2: {}\".format(roc_auc_score(y_test2, preds)))\n","print(\"MCC of RandomForest with optimal hyperparameters on test set2: {}\\n\\n\".format(matthews_corrcoef(y_test2, preds)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PLGoXDEg_g0"},"source":[""],"execution_count":null,"outputs":[]}]}